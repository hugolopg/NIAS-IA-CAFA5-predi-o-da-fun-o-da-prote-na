{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping\nfrom lightgbm import log_evaluation\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-29T21:16:53.274764Z","iopub.execute_input":"2023-05-29T21:16:53.275360Z","iopub.status.idle":"2023-05-29T21:16:55.506113Z","shell.execute_reply.started":"2023-05-29T21:16:53.275313Z","shell.execute_reply":"2023-05-29T21:16:55.504769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analisando o banco de dados","metadata":{}},{"cell_type":"code","source":"def read_fasta_file(file_path):\n    entries = []\n    with open(file_path, \"r\") as fasta_file:\n        lines = fasta_file.readlines()\n        i = 0\n        while i < len(lines):\n            if lines[i].startswith(\">\"):\n                entry = {}\n                entry['EntryID'] = lines[i][1:].split()[0]\n                \n                #entry[\"OX\"] = lines[i].split(\"OX=\")[1].split()[0]\n                i += 1\n                sequence_lines = []\n                while i < len(lines) and not lines[i].startswith(\">\"):\n                    sequence_lines.append(lines[i].strip())\n                    i += 1\n                entry['seq'] = \"\".join(sequence_lines)\n                entries.append(entry)\n            else:\n                i += 1\n    df = pd.DataFrame(entries)\n    df.set_index('EntryID', inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:16:55.508359Z","iopub.execute_input":"2023-05-29T21:16:55.508767Z","iopub.status.idle":"2023-05-29T21:16:55.520267Z","shell.execute_reply.started":"2023-05-29T21:16:55.508735Z","shell.execute_reply":"2023-05-29T21:16:55.518631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entrada\ntrainFasta = read_fasta_file('/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta')\n\n# Saída\ntrainTermsID = pd.read_csv('/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv',\n                           sep='\\t')\n# Teste\ntestFasta = read_fasta_file('/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta')\n\nprint(trainTermsID['term'].value_counts(normalize=True)*100)\nprint(trainTermsID['term'].nunique())","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:16:55.522972Z","iopub.execute_input":"2023-05-29T21:16:55.524001Z","iopub.status.idle":"2023-05-29T21:17:07.606772Z","shell.execute_reply.started":"2023-05-29T21:16:55.523964Z","shell.execute_reply":"2023-05-29T21:17:07.605516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainTermsID['aspect'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:07.609841Z","iopub.execute_input":"2023-05-29T21:17:07.610304Z","iopub.status.idle":"2023-05-29T21:17:08.499600Z","shell.execute_reply.started":"2023-05-29T21:17:07.610272Z","shell.execute_reply":"2023-05-29T21:17:08.498059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFasta","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:08.501489Z","iopub.execute_input":"2023-05-29T21:17:08.501878Z","iopub.status.idle":"2023-05-29T21:17:08.530814Z","shell.execute_reply.started":"2023-05-29T21:17:08.501848Z","shell.execute_reply":"2023-05-29T21:17:08.529493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testFasta","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:08.532246Z","iopub.execute_input":"2023-05-29T21:17:08.532640Z","iopub.status.idle":"2023-05-29T21:17:08.547161Z","shell.execute_reply.started":"2023-05-29T21:17:08.532609Z","shell.execute_reply":"2023-05-29T21:17:08.545701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainTermsID","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:08.548995Z","iopub.execute_input":"2023-05-29T21:17:08.549361Z","iopub.status.idle":"2023-05-29T21:17:08.566295Z","shell.execute_reply.started":"2023-05-29T21:17:08.549330Z","shell.execute_reply":"2023-05-29T21:17:08.564845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainTermsID['EntryID'].value_counts().mean()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:08.567867Z","iopub.execute_input":"2023-05-29T21:17:08.568224Z","iopub.status.idle":"2023-05-29T21:17:09.428283Z","shell.execute_reply.started":"2023-05-29T21:17:08.568195Z","shell.execute_reply":"2023-05-29T21:17:09.426777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFull = trainTermsID.merge(trainFasta, on='EntryID', how='left')\n#trainFull.drop('aspect', axis=1, inplace=True)\ntrainFull","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:09.429788Z","iopub.execute_input":"2023-05-29T21:17:09.430145Z","iopub.status.idle":"2023-05-29T21:17:11.862590Z","shell.execute_reply.started":"2023-05-29T21:17:09.430112Z","shell.execute_reply":"2023-05-29T21:17:11.861187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keep_most_frequent_outputs(df, percentage):\n    # Calculate output frequency\n    output_counts = df['term'].value_counts()\n\n    # Calculate the number of classes to keep based on the percentage\n    n = int(len(output_counts) * percentage)\n\n    # Get the \"n\" most frequent outputs\n    most_frequent_outputs = output_counts.head(n).index\n\n    # Filter the DataFrame to keep rows with the most frequent outputs\n    filtered_df = df[df['term'].isin(most_frequent_outputs)].copy()\n\n    # Calculate the percentage of the database removed\n    removed_percentage = (1 - len(filtered_df) / len(df)) * 100\n    print(\"Percentage of database removed: {:.2f}%\".format(removed_percentage))\n    \n    return filtered_df\n\ntrainFullD = keep_most_frequent_outputs(trainFull, 0.1)\ntrainFullD.drop('EntryID', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:11.866963Z","iopub.execute_input":"2023-05-29T21:17:11.867343Z","iopub.status.idle":"2023-05-29T21:17:15.471874Z","shell.execute_reply.started":"2023-05-29T21:17:11.867314Z","shell.execute_reply":"2023-05-29T21:17:15.470469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFullD","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:15.473516Z","iopub.execute_input":"2023-05-29T21:17:15.473894Z","iopub.status.idle":"2023-05-29T21:17:15.490268Z","shell.execute_reply.started":"2023-05-29T21:17:15.473863Z","shell.execute_reply":"2023-05-29T21:17:15.488930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separando em 3 grupos","metadata":{}},{"cell_type":"code","source":"trainBPO = trainFullD[trainFullD['aspect'] == 'BPO'].copy()\ntrainBPO.drop('aspect', axis=1, inplace=True)\ntrainCCO = trainFullD[trainFullD['aspect'] == 'CCO'].copy()\ntrainCCO.drop('aspect', axis=1, inplace=True)\ntrainMFO = trainFullD[trainFullD['aspect'] == 'MFO'].copy()\ntrainMFO.drop('aspect', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:15.491657Z","iopub.execute_input":"2023-05-29T21:17:15.491999Z","iopub.status.idle":"2023-05-29T21:17:18.947265Z","shell.execute_reply.started":"2023-05-29T21:17:15.491971Z","shell.execute_reply":"2023-05-29T21:17:18.945552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainMFO['term'].value_counts())\nprint(trainMFO['seq'].nunique())","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:18.948821Z","iopub.execute_input":"2023-05-29T21:17:18.949179Z","iopub.status.idle":"2023-05-29T21:17:19.632690Z","shell.execute_reply.started":"2023-05-29T21:17:18.949151Z","shell.execute_reply":"2023-05-29T21:17:19.631435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_most_common(df):\n    # Get value counts of the term column\n    term_counts = df['term'].value_counts()\n    \n    # Add a column to df with the count of each term\n    df['term_count'] = df['term'].map(term_counts)\n    \n    # Create an output dataframe with unique sequences\n    unique_seqs = df['seq'].unique()\n    output_df = pd.DataFrame({'seq': unique_seqs, 'term': ''})\n    \n    # Create a dictionary to store the most common term for each sequence\n    most_common_terms = {}\n    \n    # Iterate over each unique sequence with tqdm progress bar\n    for sequence in tqdm(unique_seqs, desc='Processing sequences', leave=True):\n        # Filter the df to find all rows with the sequence\n        seq_rows = df.query(\"seq == @sequence\")\n        \n        # Get the term with the highest count\n        most_common_term = seq_rows.nlargest(3, 'term_count')['term'].iloc[-1]\n        \n        # Store the most common term in the dictionary\n        most_common_terms[sequence] = most_common_term\n    \n    # Update the 'term' column in the output dataframe using the stored most common terms\n    output_df['term'] = output_df['seq'].map(most_common_terms)\n    \n    return output_df\n\ntrainBPOs = get_most_common(trainBPO)\nprint('BPO pronto')\ntrainCCOs = get_most_common(trainCCO)\nprint('CCO pronto')\ntrainMFOs = get_most_common(trainMFO)\nprint('MFO pronto')","metadata":{"execution":{"iopub.status.busy":"2023-05-29T21:17:19.633947Z","iopub.execute_input":"2023-05-29T21:17:19.634267Z","iopub.status.idle":"2023-05-30T02:47:50.339816Z","shell.execute_reply.started":"2023-05-29T21:17:19.634241Z","shell.execute_reply":"2023-05-30T02:47:50.338295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelo","metadata":{}},{"cell_type":"code","source":"# Copiando para processar\ntrainBPO = trainBPOs.copy()\ntrainCCO = trainCCOs.copy()\ntrainMFO = trainMFOs.copy()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T02:47:50.342217Z","iopub.execute_input":"2023-05-30T02:47:50.342706Z","iopub.status.idle":"2023-05-30T02:47:50.538364Z","shell.execute_reply.started":"2023-05-30T02:47:50.342662Z","shell.execute_reply":"2023-05-30T02:47:50.537039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainBPO","metadata":{"execution":{"iopub.status.busy":"2023-05-30T02:47:50.540976Z","iopub.execute_input":"2023-05-30T02:47:50.541462Z","iopub.status.idle":"2023-05-30T02:47:50.556046Z","shell.execute_reply.started":"2023-05-30T02:47:50.541413Z","shell.execute_reply":"2023-05-30T02:47:50.554147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(df):\n    # Calculate class counts\n    class_counts = df['term'].value_counts()\n    \n    # Find the class with only one sample\n    class_to_drop = class_counts[class_counts == 1].index\n    \n    if len(class_to_drop) > 0:\n        # Drop the class with only one sample from df\n        df = df[~df['term'].isin(class_to_drop)]\n    \n    gc.collect()\n    # Split the data into training and testing sets with stratification\n    Xtrain, Xval, ytrain, yval = train_test_split(df['seq'], df['term'], test_size=0.2,\n                                                  random_state=42, stratify=df['term'])\n    \n    # Preprocess the training and validation data\n    todasLetras = 'abcdefghijklmnopqrstuvwxyz'\n    vectorizer = CountVectorizer(analyzer='char', vocabulary=todasLetras)\n    Xtrain = vectorizer.fit_transform(Xtrain)\n    Xval = vectorizer.transform(Xval)\n    \n    # Convert to float64\n    Xtrain = Xtrain.astype('float64')\n    Xval = Xval.astype('float64')\n    gc.collect()\n    \n    # Create the LGBMClassifier model\n    model =  LGBMClassifier(random_state=42)\n    gc.collect()\n    # Train the model with early stopping\n    model.fit(Xtrain, ytrain,\n              callbacks=[early_stopping(100), log_evaluation(100)],\n              eval_metric='logloss',\n              eval_set=[(Xval, yval)])\n    gc.collect()\n    \n    print(model.score(Xval,yval))\n    return vectorizer, model\n\nvectBPO, modelBPO = create_model(trainBPO)\nvectCCO, modelCCO = create_model(trainCCO)\nvectMFO, modelMFO = create_model(trainMFO)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T03:09:00.701960Z","iopub.execute_input":"2023-05-30T03:09:00.702532Z","iopub.status.idle":"2023-05-30T03:13:11.461265Z","shell.execute_reply.started":"2023-05-30T03:09:00.702494Z","shell.execute_reply":"2023-05-30T03:13:11.459887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T03:13:11.463498Z","iopub.execute_input":"2023-05-30T03:13:11.463845Z","iopub.status.idle":"2023-05-30T03:13:11.709775Z","shell.execute_reply.started":"2023-05-30T03:13:11.463816Z","shell.execute_reply":"2023-05-30T03:13:11.708510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Geração da submissão","metadata":{}},{"cell_type":"code","source":"XBPO = vectBPO.transform(testFasta['seq']).astype('float64')\nXCCO = vectCCO.transform(testFasta['seq']).astype('float64')\nXMFO = vectMFO.transform(testFasta['seq']).astype('float64')\nprint('Transformação feita')\n\npredBPO = modelBPO.predict(XBPO)\npredCCO = modelCCO.predict(XCCO)\npredMFO = modelMFO.predict(XMFO)\nprint('Predição feita')\n\nprobBPO = modelBPO.predict_proba(XBPO).max(axis=1)\nprobCCO = modelCCO.predict_proba(XCCO).max(axis=1)\nprobMFO = modelMFO.predict_proba(XMFO).max(axis=1)\nprint('Probabilidade feita')","metadata":{"execution":{"iopub.status.busy":"2023-05-30T03:13:11.711220Z","iopub.execute_input":"2023-05-30T03:13:11.711619Z","iopub.status.idle":"2023-05-30T03:14:25.682555Z","shell.execute_reply.started":"2023-05-30T03:13:11.711565Z","shell.execute_reply":"2023-05-30T03:14:25.681361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combinando predições\n\n# Create separate DataFrames for each class\ndf_bpo = pd.DataFrame({'EntryID': testFasta.index.values,\n                       'Prediction': predBPO,\n                       'Probability': probBPO})\n\ndf_cco = pd.DataFrame({'EntryID': testFasta.index.values,\n                       'Prediction': predCCO,\n                       'Probability': probCCO})\n\ndf_mfo = pd.DataFrame({'EntryID': testFasta.index.values,\n                       'Prediction': predMFO,\n                       'Probability': probMFO})\n\n# Concatenate the DataFrames vertically\nsubmission_df = pd.concat([df_bpo, df_cco, df_mfo], ignore_index=True)\n\n# Save the DataFrame as submission.tsv without headers\nsubmission_df.to_csv('submission.tsv', sep='\\t', header=False, index=False)\nprint(submission_df['Prediction'].value_counts())\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2023-05-30T03:14:25.685070Z","iopub.execute_input":"2023-05-30T03:14:25.689276Z","iopub.status.idle":"2023-05-30T03:14:27.921525Z","shell.execute_reply.started":"2023-05-30T03:14:25.689229Z","shell.execute_reply":"2023-05-30T03:14:27.920218Z"},"trusted":true},"execution_count":null,"outputs":[]}]}